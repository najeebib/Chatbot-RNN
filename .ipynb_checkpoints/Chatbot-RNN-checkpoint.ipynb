{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f268e36-bf93-41d1-a830-e12fe7765902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e491b2-12bc-4007-9585-bcb3c5aef1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6981aa-e19f-4d3b-b4f8-8f08718f92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_filepath = os.path.join(\"cornell movie-dialogs corpus\", \"movie_lines.txt\")\n",
    "conv_filepath = os.path.join(\"cornell movie-dialogs corpus\", \"movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f880ecb-0dd6-4c3a-a5e7-fe4e946036f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045\tu0\tm0\tBIANCA\tThey do not!\n",
      "L1044\tu2\tm0\tCAMERON\tThey do to!\n",
      "L985\tu0\tm0\tBIANCA\tI hope so.\n",
      "L984\tu2\tm0\tCAMERON\tShe okay?\n",
      "L925\tu0\tm0\tBIANCA\tLet's go.\n",
      "L924\tu2\tm0\tCAMERON\tWow\n",
      "L872\tu0\tm0\tBIANCA\tOkay -- you're gonna need to learn how to lie.\n",
      "L871\tu2\tm0\tCAMERON\tNo\n"
     ]
    }
   ],
   "source": [
    "with open (lines_filepath, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:8]:\n",
    "        print(line.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546569e8-5f0f-4cbe-b1de-032bc5f2b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fields = [\"lineID\", \"characterID\",\"movieID\",\"character\",\"text\"]\n",
    "lines = {}\n",
    "with open (lines_filepath, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split(\"\\t\")\n",
    "        lineObj = {}\n",
    "        for i, field in enumerate(line_fields):\n",
    "            lineObj[field] = values[i]\n",
    "        lines[lineObj['lineID']] = lineObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27b4409-8508-4b29-9be7-707a4edc20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_fields = [\"character1ID\", \"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
    "conversations = []\n",
    "with open (conv_filepath, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split(\"\\t\")\n",
    "        convObj = {}\n",
    "        for i,field in enumerate(conv_fields):\n",
    "            convObj[field] = values[i]        \n",
    "        lineIds = eval(convObj[\"utteranceIDs\"])\n",
    "        split_lineIds = [re.findall(r'L\\d+', string[0]) for string in lineIds]\n",
    "        temp = []\n",
    "        for string in lineIds:\n",
    "            temp = re.findall(r'L\\d+', string)\n",
    "        \n",
    "        convObj[\"lines\"] = []\n",
    "        for lineId in temp:\n",
    "            convObj[\"lines\"].append(lines[lineId])\n",
    "        conversations. append(convObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f7064a8-68de-481b-98fc-00831b3bb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation[\"lines\"]) -1):\n",
    "        inputLine = conversation[\"lines\"][i][\"text\"].replace(\"\\r\\r\\n\",\"\").strip()\n",
    "        targetLine = conversation[\"lines\"][i+1][\"text\"].replace(\"\\r\\r\\n\",\"\").strip()\n",
    "        if inputLine and targetLine:\n",
    "            qa_pairs.append([inputLine,targetLine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e97284-9b26-43fe-ade5-1c69fa0845d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing newly formatted file...\n",
      "Done writing file\n"
     ]
    }
   ],
   "source": [
    "datafile = os.path.join(\"cornell movie-dialogs corpus\", \"formated_movie_lines.txt\")\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile,'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter)\n",
    "    for pair in qa_pairs:\n",
    "        writer.writerow(pair)\n",
    "print(\"Done writing file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4dad6d7-6f95-4bfe-a7eb-1e91b541f9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell I thought we'd start with pronunciation if that's okay with you.\\r\\r\\n\"\n",
      "b\"Well I thought we'd start with pronunciation if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\r\\n\"\n",
      "b\"No no it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\r\\n\"\n",
      "b\"Cameron.\\tThe thing is Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\r\\n\"\n",
      "b\"The thing is Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\r\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school then it was just like she got sick of it or something.\\r\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "with open(datafile, 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:8]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d541690-1757-4630-87ce-61fd612b06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0 # used for padding short sentences\n",
    "SOS_token = 1 # start of sentence token <START\n",
    "EOS_token = 2 # end of sentence token <END>\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print(\"keep words {} / {} = {:.4f}\".format(len(keep_words), len(self.word2index), len(keep_words)/len(self.word2index)))\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98de4020-fde2-42e2-b160-c74ecd301b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7998c7f-fdd6-4d8e-a7c6-fb42f31e36ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Montreal,Francoise....'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodeToAscii(\"Montréal,Françoise....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a887fdd-d632-4500-9985-e2f36b16ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\",r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\",r\" \",s)\n",
    "    s = re.sub(r\"\\s\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "187bcd5b-ed47-40f0-8426-43740f68d0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa aa s s dd'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeString(\"aa123aa!s's  dd?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96f391c8-c2b5-48f7-85b2-dadfebe36f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing file....Please wait\n",
      "Done reading\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading and processing file....Please wait\")\n",
    "lines = open(datafile, 'r', encoding='utf-8').read().strip().split('\\n')\n",
    "pairs = [[normalizeString(s) for s in pair.split('\\t')] for pair in lines ]\n",
    "print(\"Done reading\")\n",
    "voc = Vocabulary(\"cornell movie-dialogs corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f00159c-e4be-4984-b89e-5d774ee2cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "# return true if the pairs are under the threshhold\n",
    "def filterPair(p):\n",
    "    return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2574fac-0ddf-441e-bd57-2e1b0795908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 221275 pairs/conversations in the dataset)\n",
      "After filtering, there are 84210 pairs/conversations\n"
     ]
    }
   ],
   "source": [
    "pairs = [pair for pair in pairs if len(pair)>1]\n",
    "print(\"There are {} pairs/conversations in the dataset)\".format(len(pairs)))\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"After filtering, there are {} pairs/conversations\".format(len(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5109711-cb00-4ce5-83f5-82475870cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words: 21492\n",
      "['gosh if only we could find kat a boyfriend', 'let me see what i can do']\n",
      "['c esc ma tete this is my head', 'right see you re ready for the quiz']\n",
      "['that s because it s such a nice one', 'forget french']\n",
      "['there', 'where']\n",
      "['you have my word as a gentleman', 'you re sweet']\n",
      "['hi', 'looks like things worked out tonight huh']\n",
      "['you know chastity', 'i believe we share an art instructor']\n",
      "['have fun tonight', 'tons']\n",
      "['well no', 'then that s all you had to say']\n",
      "['then that s all you had to say', 'but']\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs:\n",
    "    voc.addSentence(pair[0])\n",
    "    voc.addSentence(pair[1])\n",
    "print(\"Counted words:\", voc.num_words)\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f63c5691-9d2d-4740-81cd-b404bcdd4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep words 9978 / 21489 = 0.4643\n",
      "Trimmed from 84210 pairs to 71445, 0.8484 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3\n",
    "def trimRareWords(voc,pairs, MIN_COUNT):\n",
    "    voc.trim(MIN_COUNT)\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index: \n",
    "                keep_input =False\n",
    "                break\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index: \n",
    "                keep_output =False\n",
    "                break\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs),  len(keep_pairs)/len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "pairs = trimRareWords(voc,pairs,MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c44986d-2199-402d-a89d-9ca2e2190fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cb2be4a-0ba1-472b-bfff-08497c0e279c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 32, 33, 34, 32, 35, 10, 36, 37, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexesFromSentence(voc, pairs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a82064-bed1-4a4c-ad8f-bc71e318004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gosh if only we could find kat a boyfriend', 'that s because it s such a nice one', 'there', 'you have my word as a gentleman', 'hi', 'have fun tonight', 'well no', 'then that s all you had to say', 'but', 'do you listen to this crap']\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, 4, 5, 6, 7, 8, 9, 10, 11, 2],\n",
       " [31, 32, 33, 34, 32, 35, 10, 36, 37, 2],\n",
       " [40, 2],\n",
       " [26, 42, 23, 43, 44, 10, 45, 2],\n",
       " [47, 2],\n",
       " [42, 60, 53, 2],\n",
       " [62, 63, 2],\n",
       " [64, 31, 32, 65, 26, 66, 67, 68, 2],\n",
       " [69, 2],\n",
       " [18, 26, 73, 67, 21, 74, 2]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = []\n",
    "out = []\n",
    "i = 0\n",
    "for pair in pairs[:10]:\n",
    "    inp.append(pair[0])\n",
    "    out.append(pair[1])\n",
    "print(inp)\n",
    "print(len(inp))\n",
    "indexes = [indexesFromSentence(voc, sentence) for sentence in inp]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44b4ab0e-8faf-487f-b860-1671022cda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l, fillvalue = 0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9cd87f1-7b53-4904-9c7b-019df1d3ecb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng = [len(ind) for ind in indexes]\n",
    "max(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1c40152-f4fc-4c12-9afa-0d8dcd104021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 31, 40, 26, 47, 42, 62, 64, 69, 18),\n",
       " (4, 32, 2, 42, 2, 60, 63, 31, 2, 26),\n",
       " (5, 33, 0, 23, 0, 53, 2, 32, 0, 73),\n",
       " (6, 34, 0, 43, 0, 2, 0, 65, 0, 67),\n",
       " (7, 32, 0, 44, 0, 0, 0, 26, 0, 21),\n",
       " (8, 35, 0, 10, 0, 0, 0, 66, 0, 74),\n",
       " (9, 10, 0, 45, 0, 0, 0, 67, 0, 2),\n",
       " (10, 36, 0, 2, 0, 0, 0, 68, 0, 0),\n",
       " (11, 37, 0, 0, 0, 0, 0, 2, 0, 0),\n",
       " (2, 2, 0, 0, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = zeroPadding(indexes)\n",
    "print(len(test_result))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57c4c6d7-7dc3-47c9-a722-3e61a14e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMatrix(l, values=0):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token: \n",
    "                m[i].append(0)\n",
    "            else: \n",
    "                m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59350d7a-292a-46fc-a180-cf4a114ba8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
       " [1, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       " [1, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       " [1, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       " [1, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       " [1, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_result = binaryMatrix(test_result)\n",
    "binary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01546134-bbbb-414c-abba-b37360aec9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return padded input sequence tensor and as well as a tensor of lengths for each of the sequences in the batch\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "232bda02-d5d4-4b30-a93e-aaf2213c752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the padded target sequence tensor, paddinbg mask and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "690051b2-3e57-418f-a083-0d9666c51ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch,voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4a87baa-11dd-43a1-a1b6-4f654c9dbd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input variable:\n",
      "tensor([[ 187,  111,   68,   62,  165],\n",
      "        [ 449,  696,  317,   91,   62],\n",
      "        [ 269,   26,  115, 1404,   41],\n",
      "        [  18,  157,   21,   32,   18],\n",
      "        [  26,   67,   91,   88,    6],\n",
      "        [  42,   42,  112,   86,  335],\n",
      "        [1028,  457,  685,   75,    2],\n",
      "        [ 221, 2462,    2,    2,    0],\n",
      "        [  34,    2,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths variable:\n",
      "tensor([10,  9,  8,  8,  7])\n",
      "target variable:\n",
      "tensor([[  16,   16,   16,  187,  333],\n",
      "        [ 114,  114,   95,   18,   26],\n",
      "        [ 115,  115,   34,   26,   27],\n",
      "        [  55,    2,   22,   55,   30],\n",
      "        [ 372,    0,    2,    2,  201],\n",
      "        [ 113,    0,    0,    0,  599],\n",
      "        [5788,    0,    0,    0,    2],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "max variable:\n",
      "8\n",
      "mask variable:\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "small = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small)])\n",
    "inputvar, lengths, targetvar, mask, maxtar = batches\n",
    "print(\"input variable:\")\n",
    "print(inputvar)\n",
    "print(\"lengths variable:\")\n",
    "print(lengths)\n",
    "print(\"target variable:\")\n",
    "print(targetvar)\n",
    "print(\"max variable:\")\n",
    "print(maxtar)\n",
    "print(\"mask variable:\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeece327-45bd-4a29-b0d0-67459ae2d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # initlize the GRU, input size and hidden size are set to hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # input_seq: batch of input sentences\n",
    "        # input_lenghts: kist of sentence lengths\n",
    "        # hidden state, of shape (n_layers * num_directions, batch_size hidden_size)\n",
    "        # convert word indexes to embedding\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # forward pass\n",
    "        outputs, hidden =self.gru(packed,hidden)\n",
    "        # unpack padding \n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        # outputs: output feature h_t from tha last layer\n",
    "        # hidfden: hidden state for the last timestamp\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89a38763-b617-4411-a92d-4bf140c5a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_siez = hidden_size\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # element wise multiply the current target decoder state with the encoder output and sum them\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden of shape: (1, batch size, hidden size)\n",
    "        # encoder outputs of shape: (max length, barch size, hidden size)\n",
    "\n",
    "        # calculate the attention weights\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        # transpose max length and batch size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c33c5f7-c9ff-47b1-a08d-563d85e8b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoungAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers, dropout=0.1):\n",
    "        super(LoungAttnDecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # input step: one last step of input sequence batch\n",
    "        # last hidden: final hidden layer of GRU\n",
    "        # encoder outputs: encoder models output\n",
    "        #get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # forward through undirictional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # calculate attention weights from current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # multiply attention weights to encoder to get new weighted sum context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "        # concatenate weighted context vector and GRU output\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context),1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9372c27-1e91-43a7-a975-50490c1ad0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLose(decoder_out, target, mask):\n",
    "    nTotal = mask.sum\n",
    "    target = target.view(-1,1)\n",
    "    gathered_tensor = torch.gather(decoder_out,1, target)\n",
    "    crossEntropy = - torch.log(gathered_tensor)\n",
    "    loss = crossEntropy.masked_select(mask)\n",
    "    loss = loss.mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69cb95de-04ba-46bd-8991-f786ad2f76f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input variable shape: torch.Size([9, 5])\n",
      "lengths shape: torch.Size([5])\n",
      "target variable shape: torch.Size([10, 5])\n",
      "mask shape: torch.Size([10, 5])\n",
      "max_target_len shape: 10\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lenghts, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input variable shape:\", input_variable.shape)\n",
    "print(\"lengths shape:\", lenghts.shape)\n",
    "print(\"target variable shape:\", target_variable.shape)\n",
    "print(\"mask shape:\", mask.shape)\n",
    "print(\"max_target_len shape:\", max_target_len)\n",
    "\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "attn_model = 'dot'\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "\n",
    "# define encoder and decoder\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
